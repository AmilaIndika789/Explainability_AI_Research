{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import metrics\n",
    "\n",
    "from interpret import show\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "\n",
    "# from interpret import set_visualize_provider\n",
    "# from interpret.provider import InlineProvider\n",
    "# set_visualize_provider(InlineProvider())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "df = pd.read_csv(\"./data/CEE_DATA.csv\", quotechar=\"'\")\n",
    "\n",
    "X = df[\n",
    "    [\n",
    "        \"Gender\",\n",
    "        \"Caste\",\n",
    "        \"coaching\",\n",
    "        \"time\",\n",
    "        \"Class_ten_education\",\n",
    "        \"twelve_education\",\n",
    "        \"medium\",\n",
    "        \"Class_X_Percentage\",\n",
    "        \"Class_XII_Percentage\",\n",
    "        \"Father_occupation\",\n",
    "        \"Mother_occupation\",\n",
    "    ]\n",
    "]\n",
    "Y = df[\"Performance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size Instances:  466\n",
      "Test Size Instances: 200\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "seed = 1\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.3, random_state=seed\n",
    ")\n",
    "print(\"Train Size Instances: \", X_train.shape[0])\n",
    "print(\"Test Size Instances:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size Instances:  466\n",
      "Test Size Instances: 200\n"
     ]
    }
   ],
   "source": [
    "ohe = OneHotEncoder(sparse=False)\n",
    "Xoded = pd.DataFrame(ohe.fit_transform(X),columns=ohe.get_feature_names(['Gender', 'Caste', 'coaching', 'time', 'Class_ten_education',\n",
    "       'twelve_education', 'medium', 'Class_X_Percentage',\n",
    "       'Class_XII_Percentage', 'Father_occupation', 'Mother_occupation']))\n",
    "X_train_enc, X_test_enc, Y_train_enc, Y_test_enc = train_test_split(\n",
    "    Xoded, Y, test_size=0.3, random_state=seed\n",
    ")\n",
    "print(\"Train Size Instances: \", X_train_enc.shape[0])\n",
    "print(\"Test Size Instances:\", X_test_enc.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\anaconda3\\envs\\RnD\\lib\\site-packages\\interpret\\glassbox\\ebm\\ebm.py:922: UserWarning: Multiclass is still experimental. Subject to change per release.\n",
      "  warn(\"Multiclass is still experimental. Subject to change per release.\")\n",
      "C:\\Anaconda\\anaconda3\\envs\\RnD\\lib\\site-packages\\interpret\\glassbox\\ebm\\ebm.py:925: UserWarning: Detected multiclass problem: forcing interactions to 0\n",
      "  warn(\"Detected multiclass problem: forcing interactions to 0\")\n"
     ]
    }
   ],
   "source": [
    "# https://interpret.ml/docs/ebm.html?highlight=multiclass#\n",
    "\n",
    "ebm = ExplainableBoostingClassifier(random_state=seed)\n",
    "ebm.fit(X_train_enc, Y_train_enc)\n",
    "\n",
    "# global explanations\n",
    "ebm_global = ebm.explain_global()\n",
    "show(ebm_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm_global.data(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local explanations\n",
    "ebm_local = ebm.explain_local(X_test_enc[:5], Y_train_enc[:5])\n",
    "show(ebm_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = ebm.predict(X_test_enc)\n",
    "print(\"Accuracy: \", metrics.accuracy_score(Y_test, Y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(Y_test, Y_pred, average=\"micro\"))\n",
    "print(\"Recall: \", metrics.recall_score(Y_test, Y_pred, average=\"micro\"))\n",
    "print(\"F1 score: \", metrics.f1_score(Y_test, Y_pred, average=\"micro\"))\n",
    "cm = metrics.confusion_matrix(Y_test, Y_pred, labels=ebm.classes_)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=ebm.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://interpret.ml/docs/lr.html\n",
    "\n",
    "from interpret.glassbox import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(random_state=seed)\n",
    "lr.fit(X_train_enc, Y_train_enc)\n",
    "\n",
    "lr_global = lr.explain_global()\n",
    "show(lr_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems this one doesn't support strings for y labels\n",
    "# lr_local = lr.explain_local(X_test_enc[:5], Y_test_enc[:5])\n",
    "# show(lr_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metrics(model, X_test, Y_test):\n",
    "    Y_pred = model.predict(X_test)\n",
    "    print(\"Accuracy: \", metrics.accuracy_score(Y_test, Y_pred))\n",
    "    print(\"Precision: \", metrics.precision_score(Y_test, Y_pred, average=\"micro\"))\n",
    "    print(\"Recall: \", metrics.recall_score(Y_test, Y_pred, average=\"micro\"))\n",
    "    print(\"F1 score: \", metrics.f1_score(Y_test, Y_pred, average=\"micro\"))\n",
    "    cm = metrics.confusion_matrix(Y_test, Y_pred, labels=model._model().classes_)\n",
    "    disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model._model().classes_)\n",
    "    disp.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_metrics(lr, X_test_enc, Y_test_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.glassbox import ClassificationTree\n",
    "\n",
    "dt = ClassificationTree(random_state=seed)\n",
    "dt.fit(X_train_enc, Y_train_enc)\n",
    "\n",
    "dt_global = dt.explain_global()\n",
    "show(dt_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems this one doesn't support strings for y labels\n",
    "# dt_local = dt.explain_local(X_test_enc[:5], Y_test_enc[:5])\n",
    "# show(dt_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_metrics(dt, X_test_enc, Y_test_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descision Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.glassbox import DecisionListClassifier\n",
    "\n",
    "dl = DecisionListClassifier(random_state=seed)\n",
    "dl.fit(X_train_enc, Y_train_enc)\n",
    "\n",
    "dl_global = dl.explain_global()\n",
    "show(dl_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems this one doesn't support strings for y labels\n",
    "# dl_local = dl.explain_local(X_test_enc[:5], Y_test_enc[:5])\n",
    "# show(dl_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = dl.predict(X_test_enc)\n",
    "print(\"Accuracy: \", metrics.accuracy_score(Y_test, Y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(Y_test, Y_pred, average=\"micro\"))\n",
    "print(\"Recall: \", metrics.recall_score(Y_test, Y_pred, average=\"micro\"))\n",
    "print(\"F1 score: \", metrics.f1_score(Y_test, Y_pred, average=\"micro\"))\n",
    "cm = metrics.confusion_matrix(Y_test, Y_pred, labels=dl.classes_)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=dl.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "878f638c0221e3ed571cdde73b54e3475bde5ecfcad9b387fc5f53bd43ed1e31"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
